### Read metrics about cpu usage
##[[inputs.cpu]]
##  ## Whether to report per-cpu stats or not
##  percpu = true
##  ## Whether to report total system cpu stats or not
##  totalcpu = true
##  ## If true, collect raw CPU time metrics
##  collect_cpu_time = false
##  ## If true, compute and report the sum of all non-idle CPU states
##  report_active = false
##  ## If true and the info is available then add core_id and physical_id tags
##  core_tags = false

# evtl zur normalisierung auf 1s aber ein anderer Aggregator
## [[aggregators.minmax]]
##   namepass = ["birds"]
##   ## General Aggregator Arguments:
##   ## The period on which to flush & clear the aggregator.
##   period = "1s"
##   ## If true, the original metric will be dropped by the
##   ## aggregator and will not get sent to the output plugins.
##   drop_original = false


# Read metrics from MQTT topic(s)
## V1.0#obsolet birds v1.0
## V1.0[[inputs.mqtt_consumer]]
## V1.0  servers = ["tcp://birdnet-pi-cluster.net:1883"]
## V1.0  topics = [
## V1.0    "birdnet_pi_cluster/birds/json_v2/#"
## V1.0  ]
## V1.0  data_format = "json_v2"
## V1.0  [[inputs.mqtt_consumer.topic_parsing]]
## V1.0    topic = "birdnet_pi_cluster/health/json_v2"
## V1.0  [[inputs.mqtt_consumer.json_v2]]
## V1.0    [[inputs.mqtt_consumer.json_v2.object]]
## V1.0      path = "@this"
## V1.0      tags = ["host","Samples_read","value1", "confidence", "unit"]
## V1.0      [inputs.mqtt_consumer.json_v2.object.fields]
## V1.0        confidence = "float"
## V1.0        value1 = "string"

# # Read metrics from MQTT topic(s)
# birds v2.0 pg
[[inputs.mqtt_consumer]]
  servers = ["tcp://birdnet-pi-cluster.net:1883"]
  topics = [
    "birdnet_pi_cluster/birds/#"
  ]
  data_format = "json_v2"
  [[inputs.mqtt_consumer.topic_parsing]]
    topic = "birdnet_pi_cluster/birds/+/+/+/+"
    measurement = "_/measurement/_/_/_/_"
    #tags = "_/_/country/state/city/_" host aus dem topic nicht mehr Ã¼bernehmen (duplicate column)
  [[inputs.mqtt_consumer.json_v2]]
    [[inputs.mqtt_consumer.json_v2.object]]
      path = "@this"
      tags = ["host","comname","sciname"]
      #timestamp_key = "dt"
      timestamp_format = "2006-01-02 15:04:05"

      [inputs.mqtt_consumer.json_v2.object.fields]
        confidence = "float"
        lat = "float"
        lng = "float"
        dt = "string"
        week = "int"
        sens = "float"
        cutoff = "float"
        overlap = "float"
## birds_influx# birds v2.0 influx
## birds_influx[[inputs.mqtt_consumer]]
## birds_influx  servers = ["tcp://birdnet-pi-cluster.net:1883"]
## birds_influx  topics = [
## birds_influx    "birdnet_pi_cluster/birds/#"
## birds_influx  ]
## birds_influx  data_format = "json_v2"
## birds_influx  [[inputs.mqtt_consumer.topic_parsing]]
## birds_influx    topic = "birdnet_pi_cluster/birds/+/+/+/+"
## birds_influx    measurement = "_/measurement/_/_/_/_"
## birds_influx    tags = "_/_/country/state/city/_"
## birds_influx  [[inputs.mqtt_consumer.json_v2]]
## birds_influx    [[inputs.mqtt_consumer.json_v2.object]]
## birds_influx      path = "@this"
## birds_influx      tags = ["host","comname","sciname"]
## birds_influx      timestamp_key = "dt"
## birds_influx      timestamp_format = "2006-01-02 15:04:05"
## birds_influx
## birds_influx      [inputs.mqtt_consumer.json_v2.object.fields]
## birds_influx        confidence = "float"
## birds_influx        lat = "float"
## birds_influx        lng = "float"
## birds_influx        dt = "string"
## birds_influx        week = "int"
## birds_influx        sens = "float"
## birds_influx        cutoff = "float"
## birds_influx        overlap = "float"



# health + host_staion
[[inputs.mqtt_consumer]]
  servers = ["tcp://birdnet-pi-cluster.net:1883"]
  topics = [
    "birdnet_pi_cluster/health/influx/+/system", 
    "birdnet_pi_cluster/health/influx/+/processes",
    "birdnet_pi_cluster/health/influx/+/cpu",
    "birdnet_pi_cluster/health/influx/+/mem",
    "birdnet_pi_cluster/health/influx/+/swap",
    "birdnet_pi_cluster/health/influx/+/disk",
    "birdnet_pi_cluster/health/influx/+/diskio",
    "birdnet_pi_cluster/health/influx/+/kernel",
    "birdnet_pi_cluster/health/influx/+/ping",
    "birdnet_pi_cluster/host_station/#"
  ]


## sox-n for pg
[[inputs.mqtt_consumer]]
  servers = ["tcp://birdnet-pi-cluster.net:1883"]
  topics = [
	"birdnet_pi_cluster/health/influx/+/sox-n"
  ]
  tagexclude = ["host"]
  data_format = "influx"


# # Configuration for sending metrics to InfluxDB
[[outputs.influxdb_v2]]
#   ## The URLs of the InfluxDB cluster nodes.
#   ##
#   ## Multiple URLs can be specified for a single cluster, only ONE of the
#   ## urls will be written to each interval.
#   ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
##urls = ["http://127.0.0.1:8086"]
##urls = ["http://nuc:8086"]
urls = ["http://localhost:8086"]
##urls = ["http://192.168.178.57:8086"]
#
#   ## Token for authentication.
token = "$INFLUX_TOKEN"
#
#   ## Organization is the name of the organization you wish to write to; must exist.
organization = "birds-org"
#
#   ## Destination bucket to write into.
bucket = "birds-bucket"
#
#   ## The value of this tag will be used to determine the bucket.  If this
#   ## tag is not set the 'bucket' option is used as the default.
#   # bucket_tag = ""
#
#   ## If true, the bucket tag will not be added to the metric.
#   # exclude_bucket_tag = false
#
#   ## Timeout for HTTP messages.
#   # timeout = "5s"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Proxy override, if unset values the standard proxy environment
#   ## variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## Enable or disable uint support for writing uints influxdb 2.0.
#   # influx_uint_support = false
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
namedrop = [ "birds" ]


### Read metrics about cpu usage
##[[inputs.cpu]]
##  ## Whether to report per-cpu stats or not
##  percpu = true
##  ## Whether to report total system cpu stats or not
##  totalcpu = true
##  ## If true, collect raw CPU time metrics
##  collect_cpu_time = false
##  ## If true, compute and report the sum of all non-idle CPU states
##  report_active = false
##  ## If true and the info is available then add core_id and physical_id tags
##  core_tags = false





# # Publishes metrics to a postgresql database
[[outputs.postgresql]]
#   ## Specify connection address via the standard libpq connection string:
#   ##   host=... user=... password=... sslmode=... dbname=...
#   ## Or a URL:
#   ##   postgres://[user[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]
#   ## See https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING
#   ##
#   ## All connection parameters are optional. Environment vars are also supported.
#   ## e.g. PGPASSWORD, PGHOST, PGUSER, PGDATABASE
#   ## All supported vars can be found here:
#   ##  https://www.postgresql.org/docs/current/libpq-envars.html
#   ##
#   ## Non-standard parameters:
#   ##   pool_max_conns (default: 1) - Maximum size of connection pool for parallel (per-batch per-table) inserts.
#   ##   pool_min_conns (default: 0) - Minimum size of connection pool.
#   ##   pool_max_conn_lifetime (default: 0s) - Maximum age of a connection before closing.
#   ##   pool_max_conn_idle_time (default: 0s) - Maximum idle time of a connection before closing.
#   ##   pool_health_check_period (default: 0s) - Duration between health checks on idle connections.
#   # connection = ""
connection = "postgresql://birder:birder@localhost:5432/birds"
#
#   ## Postgres schema to use.
#   # schema = "public"
schema = "stage_0"
#
#   ## Store tags as foreign keys in the metrics table. Default is false.
#   # tags_as_foreign_keys = false
#
#   ## Suffix to append to table name (measurement name) for the foreign tag table.
#   # tag_table_suffix = "_tag"
#
#   ## Deny inserting metrics if the foreign tag can't be inserted.
#   # foreign_tag_constraint = false
#
#   ## Store all tags as a JSONB object in a single 'tags' column.
#   # tags_as_jsonb = false
#
#   ## Store all fields as a JSONB object in a single 'fields' column.
#   # fields_as_jsonb = false
#
#   ## Templated statements to execute when creating a new table.
 create_templates = [
   '''CREATE TABLE {{ .table }} ({{ .columns }})''',
 ]
#   # create_templates = [
#   #   '''CREATE TABLE {{ .table }} ({{ .columns }})''',
#   # ]
#
#   ## Templated statements to execute when adding columns to a table.
#   ## Set to an empty list to disable. Points containing tags for which there is no column will be skipped. Points
#   ## containing fields for which there is no column will have the field omitted.
#   # add_column_templates = [
#   #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join ", ADD COLUMN IF NOT EXISTS " }}''',
#   # ]
#
#   ## Templated statements to execute when creating a new tag table.
#   # tag_table_create_templates = [
#   #   '''CREATE TABLE {{ .table }} ({{ .columns }}, PRIMARY KEY (tag_id))''',
#   # ]
#
#   ## Templated statements to execute when adding columns to a tag table.
#   ## Set to an empty list to disable. Points containing tags for which there is no column will be skipped.
#   # tag_table_add_column_templates = [
#   #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join ", ADD COLUMN IF NOT EXISTS " }}''',
#   # ]
#
#   ## The postgres data type to use for storing unsigned 64-bit integer values (Postgres does not have a native
#   ## unsigned 64-bit integer type).
#   ## The value can be one of:
#   ##   numeric - Uses the PostgreSQL "numeric" data type.
#   ##   uint8 - Requires pguint extension (https://github.com/petere/pguint)
#   # uint64_type = "numeric"
#
#   ## When using pool_max_conns>1, and a temporary error occurs, the query is retried with an incremental backoff. This
#   ## controls the maximum backoff duration.
#   # retry_max_backoff = "15s"
#
#   ## Approximate number of tag IDs to store in in-memory cache (when using tags_as_foreign_keys).
#   ## This is an optimization to skip inserting known tag IDs.
#   ## Each entry consumes approximately 34 bytes of memory.
#   # tag_cache_size = 100000
#
#   ## Enable & set the log level for the Postgres driver.
#   # log_level = "warn" # trace, debug, info, warn, error, none
log_level = "debug" # trace, debug, info, warn, error, none
#namepass = ["birds"]


